{"nbformat_minor": 2, "cells": [{"source": "### Import libaries", "cell_type": "markdown", "metadata": {}}, {"source": "import pandas as pd\nimport numpy as np \nimport seaborn as sns \n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport scipy.stats as ss", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "### Import data", "cell_type": "markdown", "metadata": {}}, {"source": "# Read from url - Take time to run\nurl = \"http://stat-computing.org/dataexpo/2009/2008.csv.bz2\"\ndf = pd.read_csv(url, compression='bz2')", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "# Reduce rows\ndf = df.sample(frac=0.35)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "df.head()", "cell_type": "code", "metadata": {"scrolled": true}, "outputs": [], "execution_count": null}, {"source": "Variable descriptions: http://stat-computing.org/dataexpo/2009/the-data.html\n\nReference: https://www.kaggle.com/fabiendaniel/predicting-flight-delays-tutorial", "cell_type": "markdown", "metadata": {}}, {"source": "### Data pre-processing", "cell_type": "markdown", "metadata": {}}, {"source": "df.count()", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "#Create Date column\n#df['Date'] = pd.to_datetime(df[['Year', 'Month', 'DayofMonth']])\n\n# Drop columns\n# Year: The dataset only contains data in 2008\n# Cancelled + CancellationCode + Diverted: Cancelled flight does not have delay label.\ndf.drop(['Year', 'Cancelled', 'CancellationCode', 'Diverted'], axis=1, inplace=True)\n# Delay details: Only 20% rows have these features.\ndf.drop(['CarrierDelay', 'WeatherDelay', 'NASDelay', 'SecurityDelay', 'LateAircraftDelay'], axis=1, inplace=True)\n# Time: Only keep CRSArrTime, CRSElapsedTime\ndf.drop(['DepTime', 'ArrTime', 'CRSDepTime', 'ActualElapsedTime'], axis=1, inplace=True)\n# Airport: Only keep the Dest Airport\ndf.drop(['Origin'], axis=1, inplace=True)\n# Carrier: Drop FlightNum and TailNum\ndf.drop(['FlightNum', 'TailNum'], axis=1, inplace=True)\n\ndf.dropna(inplace=True)\ndf.drop_duplicates(inplace=True)\n\ndf.count()", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "# Convert CRSArrTime to Parts of the Day\nconditions = [\n    (df['CRSArrTime'] >= 500) & (df['CRSArrTime'] < 1201),\n    (df['CRSArrTime'] >= 1201) & (df['CRSArrTime'] < 1701),\n    (df['CRSArrTime'] >= 1701) & (df['CRSArrTime'] < 2101)]\nchoices = [1, 2, 3]\ndf['PotD'] = np.select(conditions, choices, default=4)\n\n# Morning, Afternoon, Evening, Night", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "# Convert Months to Quarter\nconditions = [\n    (df['Month'] >= 4) & (df['Month'] < 7),\n    (df['Month'] >= 7) & (df['Month'] < 10),\n    (df['Month'] >= 10)]\nchoices = [2, 3, 4]\ndf['Quarter'] = np.select(conditions, choices, default=1)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "# Convert DayofMonth to Week\nconditions = [\n    (df['DayofMonth'] >= 8) & (df['DayofMonth'] < 15),\n    (df['DayofMonth'] >= 15) & (df['DayofMonth'] < 22),\n    (df['DayofMonth'] >= 22)]\nchoices = [2, 3, 4]\ndf['Week'] = np.select(conditions, choices, default=1)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "# Convert DayOfWeek to Weekend\nconditions = [\n    (df['DayOfWeek'] >= 6)]\nchoices = [1]\ndf['Weekend'] = np.select(conditions, choices, default=0)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "# Convert ArrDelay to Delay\n# A flight only counts as late if it is more than 30 minutes late.\nconditions = [\n    (df['ArrDelay'] > 30)]\nchoices = [1]\ndf['Delay'] = np.select(conditions, choices, default=0)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "df.drop(['Month', 'DayofMonth', 'DayOfWeek', 'CRSArrTime'], axis=1, inplace=True)\ndf.dtypes", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "df.head()", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "### Data analysis", "cell_type": "markdown", "metadata": {}}, {"source": "#Check basic stats\ndf.describe()", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "#Check correlation\ndf.corr()", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "# Groupby carrier\ndf.groupby(['UniqueCarrier']).mean()", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "# Groupby destination airport\ndf.groupby(['Dest']).mean()", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "# Bias check\nsns.countplot(x=\"Delay\", data=df)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "# Check Correlation\ndef cramers_corrected_stat(x, y):\n\n    \"\"\" calculate Cramers V statistic for categorial-categorial association.\n        uses correction from Bergsma and Wicher, \n        Journal of the Korean Statistical Society 42 (2013): 323-328\n    \"\"\"\n    result=-1\n    if len(x.value_counts())==1 :\n        print(\"First variable is constant\")\n    elif len(y.value_counts())==1:\n        print(\"Second variable is constant\")\n    else:   \n        conf_matrix=pd.crosstab(x, y)\n\n        if conf_matrix.shape[0]==2:\n            correct=False\n        else:\n            correct=True\n\n        chi2 = ss.chi2_contingency(conf_matrix, correction=correct)[0]\n\n        n = sum(conf_matrix.sum())\n        phi2 = chi2/n\n        r,k = conf_matrix.shape\n        phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))    \n        rcorr = r - ((r-1)**2)/(n-1)\n        kcorr = k - ((k-1)**2)/(n-1)\n        result=np.sqrt(phi2corr / min( (kcorr-1), (rcorr-1)))\n    return round(result,6)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "#### Quarter", "cell_type": "markdown", "metadata": {}}, {"source": "sns.heatmap(pd.crosstab(df['Delay'], df['Quarter'], normalize='index'))", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "cramers_corrected_stat(df['Delay'], df['Quarter'])", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "#### Week", "cell_type": "markdown", "metadata": {}}, {"source": "sns.heatmap(pd.crosstab(df['Delay'], df['Week'], normalize='index'))", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "cramers_corrected_stat(df['Delay'], df['Week'])", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "#### Weekend", "cell_type": "markdown", "metadata": {}}, {"source": "sns.heatmap(pd.crosstab(df['Delay'], df['Weekend'], normalize='index'))", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "cramers_corrected_stat(df['Delay'], df['Weekend'])", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "#### Parts of the Day", "cell_type": "markdown", "metadata": {}}, {"source": "sns.heatmap(pd.crosstab(df['Delay'], df['PotD'], normalize='index'))", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "cramers_corrected_stat(df['Delay'], df['PotD'])", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "### Encoding", "cell_type": "markdown", "metadata": {}}, {"source": "# Apply Dummy encoding to Dest column\n\n#df = pd.concat([df, pd.get_dummies(df['Dest'], prefix='Dest',dummy_na=True)],axis=1).drop(['Dest'],axis=1)\ndf.drop(['Dest'], axis=1, inplace=True) # Temp", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "# Apply Dummy encoding to UniqueCarrier column\n\ndf = pd.concat([df, pd.get_dummies(df['UniqueCarrier'], prefix='UniqueCarrier',dummy_na=True)],axis=1).drop(['UniqueCarrier'],axis=1)\ndf.head()", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "# http://www.insightsbot.com/blog/McTKK/python-one-hot-encoding-with-scikit-learn\n#from sklearn.preprocessing import LabelBinarizer\n\n#UniqueCarrier_lb = LabelBinarizer()\n#X = UniqueCarrier_lb.fit_transform(df.UniqueCarrier.values)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "### Model evaluation", "cell_type": "markdown", "metadata": {}}, {"source": "#features = df.drop(['Delay'], axis=1)\n# Why?\n# Airline:\n# Dest Airport + CRSArrTime: \n# Departure features:\n\n\nX_data = df.drop(['Delay'], axis=1)\ny_data = df['Delay']\n\n# Holdout\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=0)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "X_train.head()", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "#### Naive Bayes", "cell_type": "markdown", "metadata": {}}, {"source": "from sklearn.naive_bayes import GaussianNB\nmodel = GaussianNB()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)", "cell_type": "code", "metadata": {"scrolled": false}, "outputs": [], "execution_count": null}, {"source": "# Metrics\n# https://medium.com/thalus-ai/performance-metrics-for-classification-problems-in-machine-learning-part-i-b085d432082b\n# Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\n\nprint(confusion_matrix(y_test, y_pred))\n\n# Report\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred))", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "#### SVC", "cell_type": "markdown", "metadata": {}}, {"source": "from sklearn.svm import SVC\n\nmodel = SVC()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "# Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\n\nprint(confusion_matrix(y_test, y_pred))\n\n# Report\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred))", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "#### Logistic Regression", "cell_type": "markdown", "metadata": {}}, {"source": "from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(X, y)\n\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "# Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\n\nprint(confusion_matrix(y_test, y_pred))\n\n# Report\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred))", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "#### Ensemble learning\nhttps://medium.com/@aravanshad/gradient-boosting-versus-random-forest-cfa3fa8f0d80", "cell_type": "markdown", "metadata": {}}, {"source": "### PCA", "cell_type": "markdown", "metadata": {}}, {"source": "#Standar Scaler\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Fit on training set only.\nscaler = StandardScaler().fit(X_train)\n    \n# Apply transform to both the training set and the test set.\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "#PCA\nfrom sklearn.decomposition import PCA\n\n# Fit on training set only.\npca = PCA(n_components=1).fit(X_train)\n\n# Apply transform to both the training set and the test set.\nX_train = pca.transform(X_train)\nX_test = pca.transform(X_test)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "# TODO", "cell_type": "markdown", "metadata": {}}, {"source": "1. Edit threshhold / class weight\n2. PCA: Dimension reducing\n3. RFE: Features selection\n4. GridsearchCV for tuning\n- Tuning with 3 values\n- Goal: Precison ~ Recall\n5. Compare performances among models", "cell_type": "markdown", "metadata": {}}, {"source": "#### Note\n- Label Encoder: For algorithm that does not care about distance between data point (Naive Bayes, ...)", "cell_type": "markdown", "metadata": {}}, {"source": "", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}], "metadata": {"kernelspec": {"display_name": "Python 3.5", "name": "python3", "language": "python"}, "widgets": {"state": {}, "version": "1.1.2"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.5.5", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4}