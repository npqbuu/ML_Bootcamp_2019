{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fruit-Classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tIiIl2hEWqlq"
      },
      "source": [
        "## Setup and configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xz7oY3YcPSZe"
      },
      "source": [
        "1. Firstly, we'll import usefull packages.\n",
        "2. Then, we'll load the data, before visualize and preprocess it.\n",
        "3. We'll try a simple CNN model and then we will evaluate its performances.\n",
        "4. And finally, we'll use techniques such as data augmentation, learning rate decay and dropout to increase our model's accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C4VkgMMepiyf"
      },
      "source": [
        "**Import Packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wx9m5esyW0qh",
        "colab": {}
      },
      "source": [
        "import numpy as np                          # linear algebra\n",
        "import os                                   # used for loading the data\n",
        "from sklearn.metrics import confusion_matrix# confusion matrix to carry out error analysis\n",
        "import seaborn as sn                        # heatmap\n",
        "from sklearn.utils import shuffle           # shuffle the data\n",
        "import matplotlib.pyplot as plt             # 2D plotting library\n",
        "import tensorflow as tf                     # best library ever"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INkoGl2PE8cz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRE_MsgEC6uT",
        "colab_type": "text"
      },
      "source": [
        "Kaggle API setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7ztDLtHDGJV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "82d53afa-8177-464d-d3a7-5e5bece2133b"
      },
      "source": [
        "!pip install kaggle\n",
        "!mkdir .kaggle\n",
        "\n",
        "import json\n",
        "token = {\"username\":\"npqbuu\",\"key\":\"224292a46181101043b9eaa22bc77a39\"}\n",
        "with open('/content/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(token, file)\n",
        "\n",
        "!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json\n",
        "!kaggle config set -n path -v{/content}\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "- path is now set to: {/content}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEcZJ3JpDjmi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "2f639d3a-23a9-4c7f-c717-5d14590d10ed"
      },
      "source": [
        "# Download dataset from Kaggle\n",
        "\n",
        "!kaggle datasets download -d moltean/fruits -p /content\n",
        "!unzip \\*.zip"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading fruits.zip to /content\n",
            " 97% 690M/709M [00:04<00:00, 145MB/s]\n",
            "100% 709M/709M [00:04<00:00, 181MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VP_5dJyoD_xK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip \\*.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qpcquX3ptpJ1"
      },
      "source": [
        "**Import datas**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-KhtOc8mPKv3",
        "colab": {}
      },
      "source": [
        "# Global varriables\n",
        "train_data_dir = 'fruits-360/Training'\n",
        "test_data_dir = 'fruits-360/Test'\n",
        "\n",
        "img_height = 100\n",
        "img_width = 100\n",
        "batch_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kwF_f6aCWNv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "5201530d-6780-4962-a2ce-90ae4ac91970"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "#Load dataset\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    color_mode='rgb',\n",
        "    class_mode='categorical',\n",
        "    subset='training') # set as training data\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir, # same directory as training data\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    color_mode='rgb',\n",
        "    class_mode='categorical',\n",
        "    subset='validation') # set as validation data\n",
        "\n",
        "test_generator = train_datagen.flow_from_directory(\n",
        "    test_data_dir, # same directory as training data\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    color_mode='rgb',\n",
        "    class_mode='categorical') # set as test data"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 45852 images belonging to 114 classes.\n",
            "Found 11424 images belonging to 114 classes.\n",
            "Found 19548 images belonging to 114 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0ac9aVoCWNz",
        "colab_type": "text"
      },
      "source": [
        "**Let's explore the dataset**\n",
        "\n",
        "We can, for example, ask ourselves:\n",
        "\n",
        "- How many training and testing examples do we have ?\n",
        "- What is the size of the images ?\n",
        "- What is the proportion of each observed category ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsfUibqMCWN0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "516aad99-1d2f-4ded-dcc4-ef0d79609d5f"
      },
      "source": [
        "train_generator.image_shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 100, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-RG1SjvCWN3",
        "colab_type": "text"
      },
      "source": [
        "## Model evaluation\n",
        "Steps are:\n",
        "1. Build the model\n",
        "2. Compile the model\n",
        "3. Train / fit the data to the model\n",
        "4. Evaluate the model on the testing set\n",
        "5. Carry out an error analysis of our model.\n",
        "\n",
        "We can build an easy model composed of different layers such as:\n",
        "- Conv2D: (32 filters of size 3 by 3) The features will be \"extracted\" from the image.\n",
        "- MaxPooling2D: The images get half sized.\n",
        "- Flatten: Transforms the format of the images from a 2d-array to a 1d-array of 150 150 3 pixel values.\n",
        "- Relu : given a value x, returns max(x, 0).\n",
        "- Softmax: 6 neurons, probability that the image belongs to one of the classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_MyRXfECWN3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "outputId": "33117600-f6a7-45f1-bcc3-ac357d6b274a"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation = 'relu', input_shape = (100, 100, 3)), # the nn will learn the good filter to use\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation = 'relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dense(6, activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0726 11:38:04.677977 139689934419840 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 98, 98, 32)        896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 49, 49, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 47, 47, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 16928)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               2166912   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 6)                 774       \n",
            "=================================================================\n",
            "Total params: 2,177,830\n",
            "Trainable params: 2,177,830\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxiYekB_CWN6",
        "colab_type": "text"
      },
      "source": [
        "Then, we can compile it with some parameters such as:\n",
        "- Optimizer: adam = RMSProp + Momentum. What is Momentum and RMSProp ?\n",
        "- Momentum = takes into account past gradient to have a better update.\n",
        "- RMSProp = exponentially weighted average of the squares of past gradients.\n",
        "- Loss function: we use sparse categorical crossentropy for classification, each images belongs to one class only"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HUrK2XDCWN7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGgtqob8CWN9",
        "colab_type": "text"
      },
      "source": [
        "We fit the model to the data from the training set. The neural network will learn by itself the pattern in order to distinguish each category."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyDaEMQVCWN9",
        "colab_type": "text"
      },
      "source": [
        "Fitting/Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcHlPT9yCWN-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "outputId": "f482adc4-6d1e-4a98-c0ac-640302c4229b"
      },
      "source": [
        "STEP_SIZE_TRAIN = train_generator.n // train_generator.batch_size\n",
        "STEP_SIZE_VALID = validation_generator.n //validation_generator.batch_size\n",
        "\n",
        "model.fit_generator(generator = train_generator,\n",
        "                    steps_per_epoch = STEP_SIZE_TRAIN,\n",
        "                    validation_data = validation_generator,\n",
        "                    validation_steps = STEP_SIZE_VALID,\n",
        "                    epochs = 10\n",
        ")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-533e4bcb11e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSTEP_SIZE_VALID\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m   def evaluate_generator(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1173\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) Invalid argument: Can not squeeze dim[1], expected a dimension of 1, got 114\n\t [[{{node metrics/acc/Squeeze}}]]\n\t [[loss/mul/_83]]\n  (1) Invalid argument: Can not squeeze dim[1], expected a dimension of 1, got 114\n\t [[{{node metrics/acc/Squeeze}}]]\n0 successful operations.\n0 derived errors ignored."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOv0K2yzCWOA",
        "colab_type": "text"
      },
      "source": [
        "Save model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbQAnYgZCWOA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('temp/my_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6nGfbDSCWOD",
        "colab_type": "text"
      },
      "source": [
        "Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVm5IuCvCWOE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = load_model('temp/my_model.h5') # Load model\n",
        "\n",
        "model.evaluate_generator(generator=validation_generator, steps=STEP_SIZE_VALID)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCto7wCGCWOH",
        "colab_type": "text"
      },
      "source": [
        "Predict the output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApFtcRVzCWOH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
        "test_generator.reset()\n",
        "pred=model.predict_generator(test_generator,\n",
        "steps=STEP_SIZE_TEST,\n",
        "verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}