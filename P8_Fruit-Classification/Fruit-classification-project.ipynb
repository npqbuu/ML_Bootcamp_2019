{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fruit-Classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tIiIl2hEWqlq"
      },
      "source": [
        "## Setup and configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xz7oY3YcPSZe"
      },
      "source": [
        "References: https://github.com/pmarcelino/blog/blob/master/dogs_cats/dogs_cats.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRE_MsgEC6uT",
        "colab_type": "text"
      },
      "source": [
        "**Kaggle API setup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7ztDLtHDGJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir .kaggle\n",
        "\n",
        "import json\n",
        "token = {\"username\":\"npqbuu\",\"key\":\"224292a46181101043b9eaa22bc77a39\"}\n",
        "with open('/content/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(token, file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4CBzRKrFpFP",
        "colab_type": "code",
        "outputId": "18fe56f1-2101-43e2-98c0-e14e48ea7e52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json\n",
        "!kaggle config set -n path -v{/content}\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "- path is now set to: {/content}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEcZJ3JpDjmi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download dataset from Kaggle\n",
        "\n",
        "!kaggle datasets download -d moltean/fruits -p /content\n",
        "!unzip \\*.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qpcquX3ptpJ1"
      },
      "source": [
        "**Import datas**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-KhtOc8mPKv3",
        "colab": {}
      },
      "source": [
        "# Base variables\n",
        "base_dir = 'fruits-360/'\n",
        "train_dir = os.path.join(base_dir, 'Training')\n",
        "validation_dir = os.path.join(base_dir, 'Training')\n",
        "test_dir = os.path.join(base_dir, 'Test')\n",
        "Banana_dir = os.path.join(train_dir, 'Banana')\n",
        "Avocado_dir = os.path.join(train_dir, 'Avocado')\n",
        "\n",
        "train_size, validation_size, test_size = 200, 100, 100\n",
        "\n",
        "img_width, img_height = 224, 224  # Default input size for VGG16"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKqaRNo2UMYz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Show pictures\n",
        "import os, random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "\n",
        "def show_pictures(path):\n",
        "    random_img = random.choice(os.listdir(path))\n",
        "    img_path = os.path.join(path, random_img)\n",
        "\n",
        "    img = image.load_img(img_path, target_size=(img_width, img_height))\n",
        "    img_tensor = image.img_to_array(img)  # Image data encoded as integers in the 0â€“255 range\n",
        "    img_tensor /= 255.  # Normalize to [0,1] for plt.imshow application\n",
        "    plt.imshow(img_tensor)\n",
        "    plt.show()\n",
        "    \n",
        "for i in range(0,2):\n",
        "    show_pictures(Banana_dir)\n",
        "    show_pictures(Avocado_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGUX_NqSbivd",
        "colab_type": "text"
      },
      "source": [
        "### Extract features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRF57O5Dbw3m",
        "colab_type": "text"
      },
      "source": [
        "In this example, we want to use a solution based on pre-trained models. We will use models that are composed of two parts:\n",
        "\n",
        "- Convolutional base.\n",
        "- Classifier.\n",
        "Our approach will use the convolutional base to extract features, using them to train a classifier to classify the input image. Therefore, the features extracted from the convolutional base will be the same for all classifiers studied in this example.\n",
        "\n",
        "Now let's see how to extract features from a convolutional base."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pqcpsjirbv9Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate convolutional base\n",
        "from keras.applications import VGG16\n",
        "\n",
        "conv_base = VGG16(weights='imagenet', \n",
        "                  include_top=False,\n",
        "                  input_shape=(img_width, img_height, 3))  # 3 = number of channels in RGB pictures"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjTyZvcvcH4K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check architecture\n",
        "conv_base.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Xt8hQEjcVkS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract features\n",
        "import os, shutil\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "batch_size = 32\n",
        "\n",
        "def extract_features(directory, sample_count, subset):\n",
        "    features = np.zeros(shape=(sample_count, 7, 7, 512))  # Must be equal to the output of the convolutional base\n",
        "    labels = np.zeros(shape=(sample_count))\n",
        "    # Preprocess data\n",
        "    generator = datagen.flow_from_directory(directory,\n",
        "                                            target_size = (img_width,img_height),\n",
        "                                            batch_size = batch_size,\n",
        "                                            class_mode = 'categorical',\n",
        "                                            color_mode = 'rgb',\n",
        "                                            subset = subset)\n",
        "    # Pass data through convolutional base\n",
        "    i = 0\n",
        "    for inputs_batch, labels_batch in generator:\n",
        "        features_batch = conv_base.predict(inputs_batch)\n",
        "        features[i * batch_size: (i + 1) * batch_size] = features_batch\n",
        "        labels[i * batch_size: (i + 1) * batch_size] = labels_batch\n",
        "        i += 1\n",
        "        if i * batch_size >= sample_count:\n",
        "            break\n",
        "    return features, labels\n",
        "\n",
        "train_features, train_labels = extract_features(train_dir, train_size, 'training')  # Agree with our small dataset size\n",
        "validation_features, validation_labels = extract_features(validation_dir, validation_size, 'validation')\n",
        "test_features, test_labels = extract_features(test_dir, test_size, None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRnubacTcK_i",
        "colab_type": "text"
      },
      "source": [
        "Ok, now that we have the convolutional base, we need to pass our images through it for feature extraction."
      ]
    }
  ]
}