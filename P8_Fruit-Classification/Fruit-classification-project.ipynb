{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tIiIl2hEWqlq"
   },
   "source": [
    "## Setup and configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xz7oY3YcPSZe"
   },
   "source": [
    "1. Firstly, we'll import usefull packages.\n",
    "2. Then, we'll load the data, before visualize and preprocess it.\n",
    "3. We'll try a simple CNN model and then we will evaluate its performances.\n",
    "4. And finally, we'll use techniques such as data augmentation, learning rate decay and dropout to increase our model's accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C4VkgMMepiyf"
   },
   "source": [
    "**Import Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wx9m5esyW0qh"
   },
   "outputs": [],
   "source": [
    "import numpy as np                          # linear algebra\n",
    "import os                                   # used for loading the data\n",
    "from sklearn.metrics import confusion_matrix# confusion matrix to carry out error analysis\n",
    "import seaborn as sn                        # heatmap\n",
    "from sklearn.utils import shuffle           # shuffle the data\n",
    "import matplotlib.pyplot as plt             # 2D plotting library\n",
    "import tensorflow as tf                     # best library ever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qpcquX3ptpJ1"
   },
   "source": [
    "**Import datas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-KhtOc8mPKv3"
   },
   "outputs": [],
   "source": [
    "# Global varriables\n",
    "train_data_dir = 'temp/fruits-360/Training'\n",
    "test_data_dir = 'temp/fruits-360/Test'\n",
    "\n",
    "img_height = 100\n",
    "img_width = 100\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 45852 images belonging to 114 classes.\n",
      "Found 11424 images belonging to 114 classes.\n",
      "Found 19548 images belonging to 114 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "#Load dataset\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    subset='training') # set as training data\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir, # same directory as training data\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    subset='validation') # set as validation data\n",
    "\n",
    "test_generator = train_datagen.flow_from_directory(\n",
    "    test_data_dir, # same directory as training data\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical') # set as test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's explore the dataset**\n",
    "\n",
    "We can, for example, ask ourselves:\n",
    "\n",
    "- How many training and testing examples do we have ?\n",
    "- What is the size of the images ?\n",
    "- What is the proportion of each observed category ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.image_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "Steps are:\n",
    "1. Build the model\n",
    "2. Compile the model\n",
    "3. Train / fit the data to the model\n",
    "4. Evaluate the model on the testing set\n",
    "5. Carry out an error analysis of our model.\n",
    "\n",
    "We can build an easy model composed of different layers such as:\n",
    "- Conv2D: (32 filters of size 3 by 3) The features will be \"extracted\" from the image.\n",
    "- MaxPooling2D: The images get half sized.\n",
    "- Flatten: Transforms the format of the images from a 2d-array to a 1d-array of 150 150 3 pixel values.\n",
    "- Relu : given a value x, returns max(x, 0).\n",
    "- Softmax: 6 neurons, probability that the image belongs to one of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\npqbuu\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 98, 98, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 49, 49, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 47, 47, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 16928)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               2166912   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 2,177,830\n",
      "Trainable params: 2,177,830\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation = 'relu', input_shape = (100, 100, 3)), # the nn will learn the good filter to use\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation = 'relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(6, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can compile it with some parameters such as:\n",
    "- Optimizer: adam = RMSProp + Momentum. What is Momentum and RMSProp ?\n",
    "- Momentum = takes into account past gradient to have a better update.\n",
    "- RMSProp = exponentially weighted average of the squares of past gradients.\n",
    "- Loss function: we use sparse categorical crossentropy for classification, each images belongs to one class only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the model to the data from the training set. The neural network will learn by itself the pattern in order to distinguish each category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting/Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP_SIZE_TRAIN = train_generator.n // train_generator.batch_size\n",
    "STEP_SIZE_VALID = validation_generator.n //validation_generator.batch_size\n",
    "\n",
    "model.fit_generator(generator = train_generator,\n",
    "                    steps_per_epoch = STEP_SIZE_TRAIN,\n",
    "                    validation_data = validation_generator,\n",
    "                    validation_steps = STEP_SIZE_VALID,\n",
    "                    epochs = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('temp/my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('temp/my_model.h5') # Load model\n",
    "\n",
    "model.evaluate_generator(generator=validation_generator, steps=STEP_SIZE_VALID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
    "test_generator.reset()\n",
    "pred=model.predict_generator(test_generator,\n",
    "steps=STEP_SIZE_TEST,\n",
    "verbose=1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Fruit-Classification.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
